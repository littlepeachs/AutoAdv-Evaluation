### 3090_110上工作记录

2022年10月9日

原来我这里是预训练了一个模型，roberta-large在qqp上面预训练。

现在要在上面训一个qnli上面prompt的模型，我们希望调大一下学习率和多设置几个epoch

在qnli上面如果包含答案就是1，如果不包含就是0


2022年10月15日

对比一下qnli和qqp的性能，删除掉一个模型，尝试分别加入字符和句子的预训练，就是使用字符或者句子作为训练集，词语作为测试集
